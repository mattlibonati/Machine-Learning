{"cells":[{"cell_type":"markdown","source":["# Statistical Inference In OLS Regression"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"65d29dda-c284-45be-babd-fa905a025398","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Estimation and Inference for Ordinary Least Squares Regression"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46c3f7e6-e6fa-4404-8e3c-cdf4e04c65a7","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<b> Notes from:</b> https://www.youtube.com/watch?v=dJXkaND4x7g and https://www.youtube.com/watch?v=tSABAXaZe20 <br>\n<b>Link to latex guide:</b> https://help.gradescope.com/article/3vm6obxcyf-latex-guide#greek_letters"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bd7c876f-1457-483b-9d7d-c4c895c12653","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["A simple linear regression is the special case of an OLS regression model with a single predictor variable:\n$$ Y = \\beta_0 + \\beta_1X + \\epsilon $$\n\nFor the ith observation we will denote the regression model by:\n$$ Y = \\beta_0 + \\beta_1X_i + \\epsilon_i $$\n\nFor the random sample Y1, Y2, . . ., Yn we can estimate the parameters β0 and β1 by minimizing the sum of the squared errors:\n$$ Min \\sum_{i=1}^{n} \\epsilon_i^2 $$\n\nWhich is equivalent to mininmizing:\n$$ Min \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1X_i)^2 $$"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c2a7ddce-446d-435e-8540-331c623d3fc8","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Estimators and Estimates for Simple Linear Regression\n<br>\n* estimators get hats\n\nEstimators and Estimates for Simple Linear Regression:\nThe estimators for β0 and β1 can be computed analytically and are given by:\n\n$$ \\hat{\\beta_1} = \\frac{\\sum(Y_i - \\bar{Y})(X_i - \\bar{X})}{(X_i-\\bar{X})^2} = \\frac{Cov(Y,X)}{Var(X)} $$\n\nand \n\n$$ \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta}_1\\bar{X} $$\n\nThe regression line always goes through the centroid: $$ (\\bar{X},\\bar{Y}) $$\n\nWe refer to the formulas for β_hat0 and β_hat1 as the estimators and the values that these formulas can take for a given random sample as the estimates. In statistics we put hats on all estimators and estimates.\n\nGiven β_hat0 and β_hat1 the predicted value or fitted value is given by:\n$$ \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X $$"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"50e9a80d-a0c8-4811-b420-b9255cf9a45d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Estimation - The General Case\nWe seldom build regression models with a single predictor variable. Typically we have multiple predictor variables denoted by X1, X2, . . ., Xk, and hence the standard regression case is sometimes referred to as multiple regression in introductory regression texts. <br>\n\nWe can still think about the estimation of β0, β1, β2, . . ., βk in the same manner as the simple linear regression case:\n\n$$ Min \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - ... - \\beta_kX_{ki})^2 $$\n\nbut the computations will be performed as matrix computations."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7ac9cfcd-8988-422f-9b3b-75493851b3fc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### General Estimation - Matrix Notation\nBefore we set up the matrix formulation for the OLS model, let’s begin by defining some matrix notation:\n1. Error Vector:\n$$ \\epsilon = [\\epsilon_1 ... \\epsilon_n]^T $$\n2. Respose Vector:\n$$ Y = [Y_1 ... Y_n]^T $$\n3. Design Matrix or Predictor Matrix:\n$$ X = [1 X_1 X_2 ... X_k] $$\n4. Parameter Vector:\n$$ \\beta = [\\beta_0 \\beta_1 \\beta_2 ... \\beta_k]^T $$\n\nAll vectors are column vectors, and the superscript T denotes the vector or matrix transpose."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bd11b884-ca04-463e-b8cf-e159f66227f8","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### General Estimation - Matrix Computations\nWe minimize the sum of the squared error by minimizing:\n$$ S(\\beta) = \\epsilon^T\\epsilon $$\nwhich can be expressed as:\n$$ S(\\beta) = (Y-X\\beta)^T(Y-X\\beta) $$\nTaking the matrix derivative of S(β), we get:\n$$ S_\\beta(\\hat{\\beta})=-2x^TY+2X^Tx\\hat{\\beta} $$\nSetting the matrix derivative to zero, we can write the expression for the least squares normal equations:\n$$ X^TX\\hat{\\beta}=X^TY $$\nwhich yield the estimator\n$$ \\hat{\\beta}=(X^TX)^{-1}X^TY$$\nThe prior estimator form assumes that the inverse matrix exists and can be computed. In practice your statistical software will directly solve the normal equations using a QR factorization."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2df67bf-14f5-480f-b7d2-6b72fa6ec051","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Statistical Inference with the t-Test\n* In OLS regression the statistical inference for the individual regression coefficients can be performed by using a t-test.\n* When performing a t-test there are three primary components: (1) Stating the null and alternate hypotheses, (2) Computing the value of the test statistic, and (3) Deriving a statistical conclusion based on a desired significance level. <br>\n\n<b> Steps to Completing T-Test </b> <br>\n<b>Step 1:</b> The null and alternate hypotheses for βi are given by:\n$$ H_0 : \\beta_i = 0 \\text{ versus } H_1 : \\beta_i \\neq 0 $$\n<b>Step 2:</b> The t statistic for βi is computed by:\n$$ t_i =  \\frac{\\hat{\\beta}_i}{SE(\\hat{\\beta}_i)} $$\nand has a degrees of freedom equal to the sample size minus the number of model parameters, i.e. df = n - dim(Model). For example if you had a regression model with two predictor variables and an intercept estimated on a sample of size 50, then the t statistic would have 47 degrees of freedom. <br><br>\n<b>Step 3:</b> Reject H0 or Fail to Reject H0 based on the value of your t statistic and your significance level. This decision can be made by using the p-value of your t statistic or by using the critical value for your significance level."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"082dc4f7-c7f5-420a-a540-d4486a4a4c45","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Confidence Intervals for Parameter Estimates\nAn alternative to performing a formal hypothesis test is to use a confidence interval for your parameter estimate. There is a duality between confidence intervals and formal hypothesis testing for regression parameters.\n* The confidence interval for βˆi is given by\n$$ \\hat{\\beta}_i \\pm t(df,\\alpha/2)*SE(\\hat{\\beta}_i) $$\nWhere the t-value is : \n$$ t(df,\\alpha/2) $$\n* If the confidence interval does not contain zero, then this is equivalentto rejecting the null hypothesis <b>H0 : βi = 0</b>."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"53460080-9bde-4121-b931-a00ef02bbbda","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Statistical Intervals for Predicted Values\nThe phrase predicted value is used in statistics to refer to the in-sample fitted values from the estimated model or to refer to the out-of-sample forecasted values. The dual use of this phrase can be confusing. A better habit is use the phrases in-sample fitted values and the out-of-sample predicted values to clearly reference these different values.\n\n* Given:\n$$ \\hat{\\beta}=(X^TX)^{-1}X^TY$$\n* the vector of fitted values can be computed by\n$$ \\hat{Y} = X\\hat{\\beta} = HY $$\n* where \n$$ H = X(X^TX)^{-1}X^T $$\n* The matrix H is called the <b>hat matrix</b> since it puts the hat on Y\n* The point estimate Y_hat0 at the point x_sub0 can be computed by:\n$$ \\hat{Y}_0 = x_0^T\\hat{\\beta} $$"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"75e5f121-42c1-43d8-88d2-17db53026482","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Statistical Intervals for Predicted Values - Continued\n<br>\n* sigma_hat = error variance\n\nThe <b>confidence interval</b> for an in-sample point x_sub0 on the estimated regression function is given by:\n$$ x_0^T\\hat{\\beta} \\pm \\hat{\\sigma} \\sqrt{x_o^T(X^TX)^{-1}x_0} $$ \n\nThe <b>prediction interval</b> for the point estimate Yhat_0 for an out-of-sample x_0 is given by:\n$$ x_0^T\\hat{\\beta} \\pm \\hat{\\sigma} \\sqrt{1 + x_o^T(X^TX)^{-1}x_0} $$ \n\nNote that the out-of-sample prediction interval is always wider than the in-sample confidence interval."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e0b48c4a-e3a0-4924-9b36-cf2b4582ac2d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Further Notation and Details\nIn order to compute the t statistic you need the standard error for the parameter estimate. Most statistical software packages should provide this estimate and compute this t statistic for you. However, it is always a good idea to know from where this number comes. Here are the details needed to compute the standard error for βhat_i.\n* The estimated parameter vector βhat has the covariance matrix given by:\n$$ Cov(\\hat{\\beta}) = \\hat{\\sigma}^2X^TX $$\n* where\n$$ \\hat{\\sigma}^2 = \\frac{SSE}{n-k-1} $$\n* The variance of βhat_i is the ith diagnal element of the covariance matrix\n$$ Var(\\hat{\\beta}_i) = \\hat{\\sigma}^2(X^TX)_{ii} $$"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8a90783f-a6a5-4fb0-b529-4bf9d73928a2","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Analysis of Variance and Related Topics for Ordinary Least Squares Regression"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"03f1b2f5-2095-4505-8b7e-baaa57eb20b0","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### The ANOVA Table for OLS Regression\nThe Analysis of Variance or ANOVA Table is a fundamental output from a fitted OLS regression model. The output from the ANOVA table is used for a number of purposes: <br>\n* Show the decomposition of the total variation.\n* Compute the R-Squared and Adjusted R-Squared metrics.\n  * R2 cannot decrease when adding new ind vars, adjR2 can\n* Perform the Overall F-test for a regression effect.\n* Perform a F-test for nested models as commonly used in forward, backward, and stepwise variable selection."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e684cfa1-d45c-4416-aa57-3dce52039329","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Decomposing the Sample Variation\n* The <b>Total Sum of Squares (SST) </b> is the total variation in the sample.\n$$ \\sum{}{}_i^n(Y_i-\\bar{Y})^2 $$\n* The <b>Regression Sum of Squares (SSR)</b> is the variation in the sample that has been explained by the regression model\n$$ \\sum{}{}_i^n(\\hat{Y}_i-\\bar{Y})^2 $$\n* The <b>Error Sum of Squares (SSE)</b> is the variation in the sample that cannot be (or has not been) explained.\n$$ \\sum{}{}_i^n(Y_i-\\hat{Y})^2 $$"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8f8da56a-2187-4c71-a991-de1376f042a5","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Metrics for Goodness-Of-Fit in OLS Regression\nThe Coefficient of Determination - R-Squared:\n$$ R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} $$\n* The Coefficient of Determination R2 will take values 0 ≤ R2 ≤ 1 and represents the proportion of the variance explained by the regression model.\n+ Implicitly, R2 is a function of the number of parameters in the model. For a nested subset of predictor variables p0 < p1, i.e. p1 contains the original p0 predictor variables and some new predictor variables, R2 will have a monotonic relationship such that R2(p0) ≤ R2(p1).\n* Adjusted R-Squared: \n$$ R_{ADJ}^2 = 1 - \\frac{SSE/(n-k-1)}{SST/(n-1)}=1-\\frac{SSE/(n-p)}{SST/(n-1)} $$ \n* Note that standard regression notation uses k for the number of predictor variables included in the regression model and p for the total number of parameters in the model. When the model includes an intercept term, then p = k + 1. When the model does not include an intercept term, then p = k.\n* The Adjusted R-Squared metric accounts for the model complexity of the regression model allowing for models of different sizes to be compared.\n* The Adjusted R-Squared metric will not be monotonic in the number of model parameters."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dee87266-6ba6-4f26-894b-6bf74b114034","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### The Overall F-Test for a Regression Effect\nConsider the regression model:\n$$Y = \\beta_0+\\beta_1X_1+...+\\beta_kX_k $$\n* The Overall F-Test for a regression effect is a joint hypothesis test that at least one of the predictor variables has a non-zero coefficient.\n  * The null and alternate hypotheses are given by\n$$ H_0 : \\beta_1 = ... = \\beta_k=0 \\text{ versus }h_1 : \\beta_i \\ne 0 $$\nfor some i ∈ { 1,...,k} \n* The test statistic for the Overall F-test is given by\n$$ F_0 = \\frac{SSR/k}{SSE/(n-p)}$$\nwhich has a F-distribution with (k, n−p) degrees-of-freedom for a regression model with k predictor variables and p total parameters. When the\nregression model includes an intercept, then p = k + 1. If the regression\nmodel does not include an intercept, then p = k."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d692127a-2efe-46ec-899e-4938c420563f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### The F-Test for Nested Models\nFor our discussion of nested models let’s consider two concrete examples which we will refer to as the full model (FM):\n$$ y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 $$\nand the reduced model (RM):\n$$ y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 $$\nNotice that the predictor variables in the reduced model are a subset of the predictor variables in the full model, i.e. RM ⊂ FM:\n* In this notation we say that the FM nests the RM, or the RM is nested by the FM.\n* We only use the terms full model and reduced model in the context of nested models.\n* We can use a F-test for nested models to decide whether or not to include an additional predictor variable in the final model."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"33ce5c62-73c5-4351-908e-21c2d6cb3419","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Details for the F-Test for Nested Models\nGiven a full model and a reduced model we can perform a F-test for nested models for the exclusion of a single predictor variable or multiple predictor variables.\n<br><br>\nIn the context of our concrete example, we could test either of these null hypotheses.\n* Example 1:  Test a Single Predictor Variable\n$$ H_0 : \\beta_3 = 0  \\text{ versus }H_1 : \\beta_3 \\ne 0 $$\n* Example 1:  Test a Multiple Predictor Variables\n$$ H_0 : \\beta_2 = \\beta_3 = 0  \\text{ versus }H_1 : \\beta_i \\ne 0 $$\nfor some i ∈ {2, 3} \n<br><br>\nThe test statistic for the F-test for nested models will always have this form in terms of the FM and the RM.\n* Test Statistic for the Nested F-Test\n$$ F_0 = \\frac{[SSE(RM)-SSE(FM)]/[dim(FM)-dim(RM)]}{SSE(FM)/[n-dim(FM)]} $$\n* The test statistics is based on the reduction in the SSE obtained from adding additional predictor variables. Note that SSE(FM) is always less than SSE(RM).\n* The dimension of a statistical model is the number of parameters.\n* <b>NOTE</b> The F-Stat is a non-negative distribution. If you get a negative f-stat, numbers were plugged in wrong within the equation"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"692f90d0-531f-40cb-ab7b-f2759a1f7c1f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Connection to Forward Variable Selection\nThe F-test for nested models is the standard statistical test implemented in most statistical software packages for performing forward and backward, and hence stepwise, variable selection.\n<br><br>\nForward Variable Selection:\n* Given the model Y = β0+β1X1 and a set of candidate predictor variables Z1, . . . , Zs, how do we select the best Zi to include in our model as X2?\n* In forward variable selection the FM will be Y = β0 +β1X1 +β2Zi and the RM will be Y = β0 + β1X1. The forward variable selection algorithm will select the Zi with the largest F-statistic that is statistically significant at a predetermined level. The algorithm will continue to add predictor variables until there are no predictor variables that are statistically significant to the predetermined level."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48c4bf15-eeb6-42aa-89e6-ba98e6410856","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Connection to Backward Variable Selection\nBackward Variable Selection\n* Given the model Y = β0 + β1X1 + · · · + βsXs (11)\nhow do we eliminate predictor variables whose effects are not statistically\nsignificant?\n* In backward variable selection the FM will be Y = β0 + β1X1 + · · · + βsXs and the RM will be Y = β0 + β1X1 + · · · + βs−1Xs−1, for notational convenience. The backward variable selection algorithm will drop the Xi with the smallest F-statistic that is not statistically significant at a predetermined level. The algorithm will continue to drop predictor variables until there are no predictor variables that aren’t statistically significant to the predetermined level.\n* Note that both of the forward and backward variable selection procedures consider only one variable at each iteration."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0099953-66cb-4493-8803-873bdd54b24c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Running Regressions with R and Python"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"859a5ace-1d22-49e5-afec-54f616594e7f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Regression Analysis in Python"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ea1f284a-432f-453f-8e61-8ddb985b880f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# import Pandas and read csv file\nimport pandas as pd\ndata_raw = pd.read_csv(r'https://raw.githubusercontent.com/mattlibonati/Machine-Learning/main/datasets/HSB.csv')\n\n# data object dimensions\nprint(f'Shape of Data: {data_raw.shape}')\n\n# create a Spark DF than can be referenced by both Python and R\nspark.createDataFrame(data_raw).createOrReplaceTempView('processed') \nspark.sql('select * from processed').limit(5).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8c23846d-fa60-4b1a-b614-6abca36885c9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[2,1,1,1,3,0.29,0.88,0.67,10,33.6,43.7,40.2,39,40.6,1],[1,1,1,1,1,-0.42,0.03,0.33,2,46.9,35.9,41.9,36.3,45.6,2],[2,1,1,1,1,0.71,0.03,0.67,9,41.6,59.3,41.9,44.4,45.6,3],[2,1,2,1,3,0.06,0.03,0,15,38.9,41.1,32.7,41.7,40.6,4],[2,1,2,1,3,0.22,-0.28,0,1,36.3,48.9,39.5,41.7,45.6,5]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"sex","type":"\"long\"","metadata":"{}"},{"name":"race","type":"\"long\"","metadata":"{}"},{"name":"ses","type":"\"long\"","metadata":"{}"},{"name":"sctyp","type":"\"long\"","metadata":"{}"},{"name":"hsp","type":"\"long\"","metadata":"{}"},{"name":"locus","type":"\"double\"","metadata":"{}"},{"name":"concept","type":"\"double\"","metadata":"{}"},{"name":"mot","type":"\"double\"","metadata":"{}"},{"name":"car","type":"\"long\"","metadata":"{}"},{"name":"rdg","type":"\"double\"","metadata":"{}"},{"name":"wrtg","type":"\"double\"","metadata":"{}"},{"name":"math","type":"\"double\"","metadata":"{}"},{"name":"sci","type":"\"double\"","metadata":"{}"},{"name":"civ","type":"\"double\"","metadata":"{}"},{"name":"id","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sex</th><th>race</th><th>ses</th><th>sctyp</th><th>hsp</th><th>locus</th><th>concept</th><th>mot</th><th>car</th><th>rdg</th><th>wrtg</th><th>math</th><th>sci</th><th>civ</th><th>id</th></tr></thead><tbody><tr><td>2</td><td>1</td><td>1</td><td>1</td><td>3</td><td>0.29</td><td>0.88</td><td>0.67</td><td>10</td><td>33.6</td><td>43.7</td><td>40.2</td><td>39</td><td>40.6</td><td>1</td></tr><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>-0.42</td><td>0.03</td><td>0.33</td><td>2</td><td>46.9</td><td>35.9</td><td>41.9</td><td>36.3</td><td>45.6</td><td>2</td></tr><tr><td>2</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0.71</td><td>0.03</td><td>0.67</td><td>9</td><td>41.6</td><td>59.3</td><td>41.9</td><td>44.4</td><td>45.6</td><td>3</td></tr><tr><td>2</td><td>1</td><td>2</td><td>1</td><td>3</td><td>0.06</td><td>0.03</td><td>0</td><td>15</td><td>38.9</td><td>41.1</td><td>32.7</td><td>41.7</td><td>40.6</td><td>4</td></tr><tr><td>2</td><td>1</td><td>2</td><td>1</td><td>3</td><td>0.22</td><td>-0.28</td><td>0</td><td>1</td><td>36.3</td><td>48.9</td><td>39.5</td><td>41.7</td><td>45.6</td><td>5</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%r\nlibrary(SparkR)    # used to read Spark SQL tables (converted from Python)\n\n# convert Spark df to R df\nmydata <- SparkR::sql('Select * from processed')\nmydata <- collect(mydata)\n\n# Specify Model Params\ny<-mydata$math\nrdg<-mydata$rdg\nwrtg<-mydata$wrtg\nsci<-mydata$sci\nciv<-mydata$civ\nlocus<-mydata$locus\nconcept<-mydata$concept\nmot<-mydata$mot"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9264220d-8f61-4327-9ecf-8eb6fb48d84e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# create linear model with single predictor, rdg\n%r \nfit1<-lm(y~rdg)\nprint(summary(fit1))\nprint(anova(fit1))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c389652f-1fa6-4a53-9c21-5567df19ab8b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\nCall:\nlm(formula = y ~ rdg)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.3843  -4.8956  -0.6241   4.9568  17.7714 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 18.99507    1.47870   12.85   <2e-16 ***\nrdg          0.63300    0.02797   22.63   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 6.915 on 598 degrees of freedom\nMultiple R-squared:  0.4614,\tAdjusted R-squared:  0.4605 \nF-statistic: 512.3 on 1 and 598 DF,  p-value: < 2.2e-16\n\nAnalysis of Variance Table\n\nResponse: y\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nrdg         1  24498 24498.3  512.32 < 2.2e-16 ***\nResiduals 598  28596    47.8                      \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\nCall:\nlm(formula = y ~ rdg)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.3843  -4.8956  -0.6241   4.9568  17.7714 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 18.99507    1.47870   12.85   <2e-16 ***\nrdg          0.63300    0.02797   22.63   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 6.915 on 598 degrees of freedom\nMultiple R-squared:  0.4614,\tAdjusted R-squared:  0.4605 \nF-statistic: 512.3 on 1 and 598 DF,  p-value: < 2.2e-16\n\nAnalysis of Variance Table\n\nResponse: y\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nrdg         1  24498 24498.3  512.32 < 2.2e-16 ***\nResiduals 598  28596    47.8                      \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"]}}],"execution_count":0},{"cell_type":"code","source":["# create multi linear model with rdg, wrtg, sci, civ, locus, concept, and mot\n%r\nfit3<-lm(y~rdg + wrtg + sci + civ + locus + concept + mot)\nprint(summary(fit3))\nprint(anova(fit3))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a97475f8-32c0-45bc-b8cd-9ccd2a9ef5e4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\nCall:\nlm(formula = y ~ rdg + wrtg + sci + civ + locus + concept + mot)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.5131  -4.0713  -0.3125   4.2712  18.6962 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  8.42029    1.77143   4.753 2.51e-06 ***\nrdg          0.26427    0.03942   6.704 4.74e-11 ***\nwrtg         0.23644    0.03704   6.383 3.50e-10 ***\nsci          0.25521    0.03771   6.767 3.16e-11 ***\nciv          0.07147    0.03405   2.099   0.0363 *  \nlocus        0.45117    0.42276   1.067   0.2863    \nconcept      0.01729    0.37974   0.046   0.9637    \nmot          0.53436    0.81137   0.659   0.5104    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 6.19 on 592 degrees of freedom\nMultiple R-squared:  0.5728,\tAdjusted R-squared:  0.5678 \nF-statistic: 113.4 on 7 and 592 DF,  p-value: < 2.2e-16\n\nAnalysis of Variance Table\n\nResponse: y\n           Df  Sum Sq Mean Sq  F value    Pr(>F)    \nrdg         1 24498.3 24498.3 639.4555 < 2.2e-16 ***\nwrtg        1  3713.3  3713.3  96.9253 < 2.2e-16 ***\nsci         1  1947.6  1947.6  50.8373 2.936e-12 ***\nciv         1   179.1   179.1   4.6749   0.03101 *  \nlocus       1    56.4    56.4   1.4729   0.22538    \nconcept     1     2.2     2.2   0.0566   0.81200    \nmot         1    16.6    16.6   0.4337   0.51042    \nResiduals 592 22680.2    38.3                       \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\nCall:\nlm(formula = y ~ rdg + wrtg + sci + civ + locus + concept + mot)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.5131  -4.0713  -0.3125   4.2712  18.6962 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  8.42029    1.77143   4.753 2.51e-06 ***\nrdg          0.26427    0.03942   6.704 4.74e-11 ***\nwrtg         0.23644    0.03704   6.383 3.50e-10 ***\nsci          0.25521    0.03771   6.767 3.16e-11 ***\nciv          0.07147    0.03405   2.099   0.0363 *  \nlocus        0.45117    0.42276   1.067   0.2863    \nconcept      0.01729    0.37974   0.046   0.9637    \nmot          0.53436    0.81137   0.659   0.5104    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 6.19 on 592 degrees of freedom\nMultiple R-squared:  0.5728,\tAdjusted R-squared:  0.5678 \nF-statistic: 113.4 on 7 and 592 DF,  p-value: < 2.2e-16\n\nAnalysis of Variance Table\n\nResponse: y\n           Df  Sum Sq Mean Sq  F value    Pr(>F)    \nrdg         1 24498.3 24498.3 639.4555 < 2.2e-16 ***\nwrtg        1  3713.3  3713.3  96.9253 < 2.2e-16 ***\nsci         1  1947.6  1947.6  50.8373 2.936e-12 ***\nciv         1   179.1   179.1   4.6749   0.03101 *  \nlocus       1    56.4    56.4   1.4729   0.22538    \nconcept     1     2.2     2.2   0.0566   0.81200    \nmot         1    16.6    16.6   0.4337   0.51042    \nResiduals 592 22680.2    38.3                       \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"]}}],"execution_count":0},{"cell_type":"code","source":["# create multi linear model with rdg, wrtg, sci, and civ\n\n%r\nfit4<-lm(y~rdg + wrtg + sci + civ)\nprint(summary(fit4))\nprint(anova(fit4))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48e99445-aa16-4091-9186-c2d908861042","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\nCall:\nlm(formula = y ~ rdg + wrtg + sci + civ)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.4544  -4.1609  -0.2551   4.3703  18.6438 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.79236    1.62873   4.784 2.17e-06 ***\nrdg          0.27245    0.03895   6.996 7.13e-12 ***\nwrtg         0.24584    0.03620   6.791 2.71e-11 ***\nsci          0.25532    0.03733   6.839 1.98e-11 ***\nciv          0.07342    0.03393   2.164   0.0309 *  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 6.184 on 595 degrees of freedom\nMultiple R-squared:  0.5714,\tAdjusted R-squared:  0.5685 \nF-statistic: 198.3 on 4 and 595 DF,  p-value: < 2.2e-16\n\nAnalysis of Variance Table\n\nResponse: y\n           Df  Sum Sq Mean Sq F value    Pr(>F)    \nrdg         1 24498.3 24498.3 640.572 < 2.2e-16 ***\nwrtg        1  3713.3  3713.3  97.094 < 2.2e-16 ***\nsci         1  1947.6  1947.6  50.926 2.802e-12 ***\nciv         1   179.1   179.1   4.683   0.03086 *  \nResiduals 595 22755.4    38.2                      \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\nCall:\nlm(formula = y ~ rdg + wrtg + sci + civ)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.4544  -4.1609  -0.2551   4.3703  18.6438 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.79236    1.62873   4.784 2.17e-06 ***\nrdg          0.27245    0.03895   6.996 7.13e-12 ***\nwrtg         0.24584    0.03620   6.791 2.71e-11 ***\nsci          0.25532    0.03733   6.839 1.98e-11 ***\nciv          0.07342    0.03393   2.164   0.0309 *  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 6.184 on 595 degrees of freedom\nMultiple R-squared:  0.5714,\tAdjusted R-squared:  0.5685 \nF-statistic: 198.3 on 4 and 595 DF,  p-value: < 2.2e-16\n\nAnalysis of Variance Table\n\nResponse: y\n           Df  Sum Sq Mean Sq F value    Pr(>F)    \nrdg         1 24498.3 24498.3 640.572 < 2.2e-16 ***\nwrtg        1  3713.3  3713.3  97.094 < 2.2e-16 ***\nsci         1  1947.6  1947.6  50.926 2.802e-12 ***\nciv         1   179.1   179.1   4.683   0.03086 *  \nResiduals 595 22755.4    38.2                      \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Estimation of Inference in OLS Regression","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2980685845577765}},"nbformat":4,"nbformat_minor":0}
